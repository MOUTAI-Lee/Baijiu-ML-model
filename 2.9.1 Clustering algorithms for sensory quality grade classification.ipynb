{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74c5d0ee",
   "metadata": {},
   "source": [
    "##  2.9.1 Clustering algorithms for sensory quality grade classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "# Assume data has been loaded into a dataframe named df\n",
    "# df = pd.read_csv('data.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "# Select relevant features\n",
    "X = df.drop(columns=['target'])  # Assuming 'target' is the target column\n",
    "y = df['target']  # If there's a target column\n",
    "\n",
    "# Select the top K best features using ANOVA F-test\n",
    "X_new = SelectKBest(f_classif, k=10).fit_transform(X, y)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_new)\n",
    "\n",
    "# Outlier detection and removal (5% of outliers)\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "outliers = iso_forest.fit_predict(X_scaled)\n",
    "X_scaled = X_scaled[outliers == 1]  # Retain only normal data points\n",
    "\n",
    "# 2.9.1 K-means Clustering\n",
    "# Use the elbow method to determine the optimal number of clusters\n",
    "wcss = []  # Within-cluster sum of squares (WCSS)\n",
    "for k in range(1, 11):  # k from 1 to 10\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, tol=1e-4, n_init=10, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow method graph\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# From the elbow method, assume the optimal number of clusters k=3\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, tol=1e-4, n_init=10, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# 2.9.2 Gaussian Mixture Model (GMM) Clustering\n",
    "# Perform clustering with GMM, where k is the number of clusters\n",
    "gmm = GaussianMixture(n_components=k, covariance_type='full', max_iter=100, tol=1e-3, random_state=42, reg_covar=1e-6)\n",
    "gmm_labels = gmm.fit_predict(X_scaled)\n",
    "\n",
    "# 2.9.3 Hierarchical Clustering (HCA)\n",
    "# Perform hierarchical clustering, where k is the number of clusters\n",
    "hca = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='ward')\n",
    "hca_labels = hca.fit_predict(X_scaled)\n",
    "\n",
    "# 2.9.4 Clustering Evaluation\n",
    "# Evaluate clustering performance using Calinski-Harabasz index and Davies-Bouldin index\n",
    "\n",
    "# K-means evaluation\n",
    "ch_score_kmeans = calinski_harabasz_score(X_scaled, kmeans_labels)\n",
    "dbi_score_kmeans = davies_bouldin_score(X_scaled, kmeans_labels)\n",
    "\n",
    "# GMM evaluation\n",
    "ch_score_gmm = calinski_harabasz_score(X_scaled, gmm_labels)\n",
    "dbi_score_gmm = davies_bouldin_score(X_scaled, gmm_labels)\n",
    "\n",
    "# HCA evaluation\n",
    "ch_score_hca = calinski_harabasz_score(X_scaled, hca_labels)\n",
    "dbi_score_hca = davies_bouldin_score(X_scaled, hca_labels)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f'K-means - Calinski-Harabasz Index: {ch_score_kmeans}, Davies-Bouldin Index: {dbi_score_kmeans}')\n",
    "print(f'GMM - Calinski-Harabasz Index: {ch_score_gmm}, Davies-Bouldin Index: {dbi_score_gmm}')\n",
    "print(f'HCA - Calinski-Harabasz Index: {ch_score_hca}, Davies-Bouldin Index: {dbi_score_hca}')\n",
    "\n",
    "# Visualize clustering results\n",
    "# Plot a scatter plot for K-means clustering results (using the first two features)\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans_labels, cmap='viridis')\n",
    "plt.title('K-means Clustering Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "# Similarly, you can visualize GMM and HCA results as well.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eli5_env",
   "language": "python",
   "name": "eli5_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
